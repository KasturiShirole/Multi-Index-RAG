{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rgBPk4Ye2Q93"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U \\\n",
        "     Sentence-transformers==3.0.1 \\\n",
        "     langchain==0.3.19 \\\n",
        "     langchain-groq==0.2.4 \\\n",
        "     langchain-chroma==0.2.2 \\\n",
        "     langchain-community==0.3.18 \\\n",
        "     langchain-huggingface==0.1.2 \\\n",
        "     einops==0.8.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from chromadb.config import Settings\n",
        "import chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm34AbLd2oMk",
        "outputId": "68e23456-13d7-4c4f-f8a0-6bda15a4540b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass"
      ],
      "metadata": {
        "id": "CgO5F0t320Rh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhf0fpqH24o_",
        "outputId": "f9605cb8-db74-436c-8099-df05d32d13d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFHap2Ao3W3k",
        "outputId": "2dd4e6b9-8d55-4f0e-c390-9cc3b61f6d9b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 : Load and preprocess data"
      ],
      "metadata": {
        "id": "1prvbjfn3iB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_process_data(url):\n",
        "    # Load data from web\n",
        "    loader = WebBaseLoader(url)\n",
        "    data = loader.load()\n",
        "\n",
        "    # Split text into chunks (Experiment with Chunk Size and Chunk Overlap to get optimal chunking)\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    chunks = text_splitter.split_documents(data)\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "xJA0F6PB3cQ4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Create multiple vector stores code"
      ],
      "metadata": {
        "id": "9zzOBFVg3xWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_stores(chunks_list):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"nomic-ai/nomic-embed-text-v1.5\", model_kwargs={'trust_remote_code': True})\n",
        "\n",
        "    # 1. Define settings (this is correct)\n",
        "    client_settings = Settings(anonymized_telemetry=False)\n",
        "\n",
        "    # 2. Create the client ONCE, outside the loop\n",
        "    # We use .Client() for an in-memory \"ephemeral\" client\n",
        "    client = chromadb.Client(client_settings)\n",
        "\n",
        "    vector_stores = []\n",
        "\n",
        "    # 3. Loop and create collections\n",
        "    for i, chunks in enumerate(chunks_list):\n",
        "\n",
        "        # Create a unique name\n",
        "        collection_name = f\"source_collection_{i}\"\n",
        "\n",
        "        # 4. Pass the EXISTING client and a unique name.\n",
        "        #    DO NOT pass client_settings here again.\n",
        "        vector_store = Chroma.from_documents(\n",
        "            chunks,\n",
        "            embeddings,\n",
        "            client=client,  # <-- Pass the client you already made\n",
        "            collection_name=collection_name  # <-- Pass the unique name\n",
        "        )\n",
        "        vector_stores.append(vector_store)\n",
        "\n",
        "    return vector_stores"
      ],
      "metadata": {
        "id": "vZSMTtUn3phw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Mult-index-RAG related code\n",
        "\n",
        "1. **Retrieval from Multiple Indices**: We retrieve relevant documents from each vector store.\n",
        "2. **Context Combination:** We combine the retrieved documents from all sources into a single context.\n",
        "3. **Response Generation:** Using the combined context, we generate a final response to the original query.\n",
        "4. **Source Usage Explanation:** We generate an explanation of how information from different sources was used to answer the question."
      ],
      "metadata": {
        "id": "zFi9uDHi35Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_index_rag(query, vector_stores, llm):\n",
        "    # Retrieve documents from each vector store\n",
        "    all_docs = []\n",
        "    for i, vector_store in enumerate(vector_stores):\n",
        "        docs = vector_store.similarity_search(query, k=3)  # Increased k to 3\n",
        "        all_docs.extend([f\"Source {i+1}: \" + doc.page_content for doc in docs])\n",
        "\n",
        "    # Combine retrieved documents\n",
        "    context = \"\\n\\n\".join(all_docs)\n",
        "\n",
        "    # Generate response using combined context\n",
        "    response_prompt = ChatPromptTemplate.from_template(\n",
        "        \"You are an AI assistant tasked with answering questions based on the provided context. \"\n",
        "        \"The context contains information from multiple sources related to AI, machine learning, and NLP. \"\n",
        "        \"Please analyze the context carefully and provide a comprehensive answer to the question. \"\n",
        "        \"If the context doesn't contain enough information, use your general knowledge to supplement the answer, \"\n",
        "        \"but prioritize information from the context when available.\\n\\n\"\n",
        "        \"Context:\\n{context}\\n\\n\"\n",
        "        \"Question: {query}\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "    response_chain = response_prompt | llm\n",
        "    try:\n",
        "        response = response_chain.invoke({\"context\": context, \"query\": query})\n",
        "        final_answer = response.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating response: {e}\")\n",
        "        final_answer = \"I apologize, but I encountered an error while generating the response.\"\n",
        "\n",
        "    # Generate explanation of source usage\n",
        "    explanation_prompt = ChatPromptTemplate.from_template(\n",
        "        \"Based on the answer you provided, explain how information from different sources was used to answer the question. \"\n",
        "        \"If general knowledge was used to supplement the answer, mention that as well.\\n\\n\"\n",
        "        \"Context: {context}\\n\"\n",
        "        \"Question: {query}\\n\"\n",
        "        \"Answer: {answer}\\n\"\n",
        "        \"Explanation:\"\n",
        "    )\n",
        "    explanation_chain = explanation_prompt | llm\n",
        "    try:\n",
        "        explanation = explanation_chain.invoke({\"context\": context, \"query\": query, \"answer\": final_answer})\n",
        "        source_explanation = explanation.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating explanation: {e}\")\n",
        "        source_explanation = \"Unable to generate explanation due to an error.\"\n",
        "\n",
        "    return {\n",
        "        \"final_answer\": final_answer,\n",
        "        \"source_explanation\": source_explanation,\n",
        "        \"retrieved_context\": context\n",
        "    }"
      ],
      "metadata": {
        "id": "OtHsGUWi4LTw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Create chunk of web data to Chroma Vector Store"
      ],
      "metadata": {
        "id": "LJ9bhSIT4XbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall numpy pandas scipy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xxcJvHMV5JYG",
        "outputId": "357352fb-0d64-4aa1-a1c2-86e100f40240"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Downloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy\n",
            "  Downloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m135.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m120.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m163.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.4/308.4 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, tzdata, threadpoolctl, six, numpy, joblib, scipy, python-dateutil, scikit-learn, pandas\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.6.0\n",
            "    Uninstalling threadpoolctl-3.6.0:\n",
            "      Successfully uninstalled threadpoolctl-3.6.0\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.2\n",
            "    Uninstalling joblib-1.5.2:\n",
            "      Successfully uninstalled joblib-1.5.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.3\n",
            "    Uninstalling scipy-1.16.3:\n",
            "      Successfully uninstalled scipy-1.16.3\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-chroma 0.2.2 requires numpy<2.0.0,>=1.26.2; python_version >= \"3.12\", but you have numpy 2.3.4 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.5.2 numpy-2.3.4 pandas-2.3.3 python-dateutil-2.9.0.post0 pytz-2025.2 scikit-learn-1.7.2 scipy-1.16.3 six-1.17.0 threadpoolctl-3.6.0 tzdata-2025.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "numpy",
                  "six"
                ]
              },
              "id": "ca887b783a824bf0b7829cb039da9b17"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0.5\n",
        ")\n",
        "\n",
        "# Load and process data\n",
        "urls = [\n",
        "        \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
        "        \"https://en.wikipedia.org/wiki/Machine_learning\",\n",
        "        \"https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
        "      ]\n",
        "\n",
        "print(\"Loading and processing data...\")\n",
        "chunks_list = [load_and_process_data(url) for url in urls]\n",
        "\n",
        "# Create multiple vector stores\n",
        "print(\"Creating vector stores...\")\n",
        "vector_stores = create_vector_stores(chunks_list)\n",
        "print(\"Vector stores created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9EU58qU4YHc",
        "outputId": "e2630d6a-ed48-49e7-d3b3-5860bcc0de1a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and processing data...\n",
            "Creating vector stores...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.nomic_hyphen_ai.nomic_hyphen_bert_hyphen_2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert:<All keys matched successfully>\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector stores created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Run Multi-index RAG\n",
        "\n",
        "This implementation shows the key parts of Multi-index RAG:\n",
        "\n",
        "1. Use of multiple vector stores representing different sources or types of information\n",
        "2. Retrieval and combination of information from multiple sources\n",
        "Generation of a comprehensive response using the combined information\n",
        "3. Explanation of how different sources contributed to the answer"
      ],
      "metadata": {
        "id": "yXzAf7jk6NxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "query = \"How do AI, machine learning, and NLP relate to each other in modern applications?\"\n",
        "\n",
        "# Run Multi-index RAG\n",
        "result = multi_index_rag(query, vector_stores, llm)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"\\nFinal Answer:\")\n",
        "print(result[\"final_answer\"])\n",
        "print(\"\\nSource Usage Explanation:\")\n",
        "print(result[\"source_explanation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUJL5Iue6JbB",
        "outputId": "b6b0115a-e5c8-48c8-92f7-fefeb6d9217e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: How do AI, machine learning, and NLP relate to each other in modern applications?\n",
            "\n",
            "Final Answer:\n",
            "Based on the provided context, AI, machine learning, and NLP are closely related concepts that have evolved together in modern applications. Here's a comprehensive overview of their relationship:\n",
            "\n",
            "1. **AI and Machine Learning**: Machine learning is a subfield of AI that focuses on developing programs that can improve their performance on a given task automatically. As mentioned in the context, machine learning has been a part of AI from the beginning and has been a driving force behind the success of AI research in recent years.\n",
            "\n",
            "2. **Machine Learning and NLP**: Machine learning is a key enabler of NLP, which involves enabling computers to process, understand, and generate human language. As mentioned in Source 3, NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, and machine learning approaches, including statistical and neural networks, have many advantages over the symbolic approach in NLP.\n",
            "\n",
            "3. **NLP and AI**: NLP is a key application of AI, and it has been a focus area for AI research for decades. NLP involves developing computer systems that can converse with humans, understand natural language, and generate human-like responses. The context mentions that NLP finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.\n",
            "\n",
            "4. **Document AI and NLP**: Document AI is a platform that sits on top of NLP technology, enabling users to quickly train a computer to extract specific data from different document types. This highlights the close relationship between NLP and AI, where NLP is used to enable AI systems to understand and process human language.\n",
            "\n",
            "In modern applications, AI, machine learning, and NLP are often used together to achieve complex tasks such as:\n",
            "\n",
            "* **Document analysis**: AI and machine learning are used to analyze documents, while NLP is used to extract specific data from the documents.\n",
            "* **Conversational systems**: AI and machine learning are used to develop conversational systems that can converse with humans, while NLP is used to understand and generate human language.\n",
            "* **Predictive analytics**: Machine learning is used to develop predictive models, while NLP is used to analyze text data and extract insights.\n",
            "\n",
            "In summary, AI, machine learning, and NLP are closely related concepts that have evolved together in modern applications. Machine learning is a key enabler of NLP, and NLP is a key application of AI. The combination of AI, machine learning, and NLP enables the development of complex systems that can analyze and process human language, understand natural language, and generate human-like responses.\n",
            "\n",
            "Source Usage Explanation:\n",
            "To answer the question, I used information from multiple sources to provide a comprehensive overview of the relationship between AI, machine learning, and NLP in modern applications.\n",
            "\n",
            "**Source 1: Learning** provided the foundation for understanding machine learning as a subfield of AI that focuses on developing programs that can improve their performance on a given task automatically.\n",
            "\n",
            "**Source 2: As a scientific endeavour** provided additional context on the history of machine learning, its relationship with AI, and its applications in various fields, including natural language processing.\n",
            "\n",
            "**Source 2: ML finds application in many fields** further highlighted the applications of machine learning in various fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.\n",
            "\n",
            "**Source 3: Document AI** provided a practical example of how NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents.\n",
            "\n",
            "**Source 3: Approaches** provided a detailed overview of the different approaches used in NLP, including symbolic, statistical, and neural networks.\n",
            "\n",
            "To supplement the information from these sources, I used general knowledge to understand the relationships between AI, machine learning, and NLP. I also used general knowledge to provide examples of how these concepts are used together in modern applications, such as document analysis, conversational systems, and predictive analytics.\n",
            "\n",
            "Overall, the answer was constructed by combining information from multiple sources, general knowledge, and logical reasoning to provide a comprehensive overview of the relationship between AI, machine learning, and NLP in modern applications.\n"
          ]
        }
      ]
    }
  ]
}